{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suprimir warnings TensorFlow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# PyDMD\n",
    "from pydmd import DMD\n",
    "\n",
    "# M√≥dulos propios\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.metrics import evaluate_all\n",
    "\n",
    "# Configuraci√≥n\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('deep')\n",
    "\n",
    "DATA_DIR = Path('../data/processed')\n",
    "MODEL_DIR = Path('../data/models')\n",
    "FIG_DIR = Path('../reports/figures')\n",
    "RESULTS_DIR = DATA_DIR / 'experiments'\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Verificar GPU\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üîß GPUs disponibles: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"   GPU: {tf.config.list_physical_devices('GPU')[0].name}\")\n",
    "\n",
    "print(f\"\\nüìÅ Directorios:\")\n",
    "print(f\"   Data: {DATA_DIR}\")\n",
    "print(f\"   Models: {MODEL_DIR}\")\n",
    "print(f\"   Experiments: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa32f9",
   "metadata": {},
   "source": [
    "## **1. Cargar Datos Base**\n",
    "\n",
    "Reutilizar el pipeline de datos del notebook 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos procesados ERA5\n",
    "nc_path = DATA_DIR / 'era5_precipitation_chile_full.nc'\n",
    "ds = xr.open_dataset(nc_path)\n",
    "ds_daily = ds.resample(time='1D').sum('time') * 1000  # m/d√≠a ‚Üí mm/d√≠a\n",
    "\n",
    "precip_data = ds_daily['tp'].values  # (366, 157, 41)\n",
    "n_days, n_lat, n_lon = precip_data.shape\n",
    "\n",
    "print(f\"‚úÖ Datos cargados:\")\n",
    "print(f\"   Shape: {precip_data.shape}\")\n",
    "print(f\"   D√≠as: {n_days}, Grid: {n_lat}√ó{n_lon}\")\n",
    "print(f\"   Media: {precip_data.mean():.2f} mm/d√≠a\")\n",
    "print(f\"   Std: {precip_data.std():.2f} mm/d√≠a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d052c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar datos\n",
    "precip_flat = precip_data.reshape(n_days, -1)\n",
    "scaler = StandardScaler()\n",
    "precip_normalized = scaler.fit_transform(precip_flat)\n",
    "X = precip_normalized.reshape(n_days, n_lat, n_lon, 1)\n",
    "\n",
    "print(f\"‚úÖ Normalizaci√≥n completada:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   X mean: {X.mean():.6f} (cercano a 0)\")\n",
    "print(f\"   X std: {X.std():.6f} (cercano a 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85907b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear secuencias temporales\n",
    "WINDOW_SIZE = 7\n",
    "\n",
    "def create_sequences(data, window_size=7):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X_seq.append(data[i:i+window_size])\n",
    "        y_seq.append(data[i+window_size])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X, WINDOW_SIZE)\n",
    "\n",
    "# Split train/val/test\n",
    "n_train = int(0.7 * len(X_seq))\n",
    "n_val = int(0.15 * len(X_seq))\n",
    "\n",
    "X_train = X_seq[:n_train]\n",
    "y_train = y_seq[:n_train]\n",
    "X_val = X_seq[n_train:n_train+n_val]\n",
    "y_val = y_seq[n_train:n_train+n_val]\n",
    "X_test = X_seq[n_train+n_val:]\n",
    "y_test = y_seq[n_train+n_val:]\n",
    "\n",
    "print(f\"‚úÖ Secuencias creadas:\")\n",
    "print(f\"   Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "print(f\"   Window size: {WINDOW_SIZE} d√≠as\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95924bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar pesos espaciales (kriging variance)\n",
    "kriging_ds = xr.open_dataset(DATA_DIR / 'precipitation_kriging_june2020.nc')\n",
    "kriging_variance = kriging_ds['kriging_variance'].values\n",
    "spatial_weights = 1 / (kriging_variance + 1e-6)  # Inversa de varianza\n",
    "spatial_weights = spatial_weights / spatial_weights.max()  # Normalizar [0, 1]\n",
    "weights_tensor = tf.constant(spatial_weights, dtype=tf.float32)\n",
    "\n",
    "print(f\"‚úÖ Pesos espaciales cargados:\")\n",
    "print(f\"   Shape: {spatial_weights.shape}\")\n",
    "print(f\"   Range: [{spatial_weights.min():.3f}, {spatial_weights.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0312836",
   "metadata": {},
   "source": [
    "## **2. Funci√≥n de Experimento Automatizada**\n",
    "\n",
    "Wrapper para entrenar y evaluar modelos con diferentes configuraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(latent_dim=64, dilations=[1,2,4,8], l2_reg=0.0001):\n",
    "    \"\"\"Construir autoencoder con par√°metros configurables.\"\"\"\n",
    "    input_shape = (n_lat, n_lon, 1)\n",
    "    \n",
    "    # ENCODER\n",
    "    encoder_input = keras.Input(shape=input_shape, name='spatial_input')\n",
    "    x = encoder_input\n",
    "    \n",
    "    for i, dilation in enumerate(dilations):\n",
    "        x = layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            dilation_rate=dilation,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l2(l2_reg),\n",
    "            name=f'conv{i+1}_d{dilation}'\n",
    "        )(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    encoded = layers.Dense(\n",
    "        latent_dim,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(l2_reg),\n",
    "        name='latent'\n",
    "    )(x)\n",
    "    \n",
    "    encoder = keras.Model(encoder_input, encoded, name='encoder')\n",
    "    \n",
    "    # DECODER\n",
    "    decoder_input = keras.Input(shape=(latent_dim,), name='latent_input')\n",
    "    x = layers.Dense(\n",
    "        n_lat * n_lon,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(l2_reg)\n",
    "    )(decoder_input)\n",
    "    x = layers.Reshape((n_lat, n_lon, 1))(x)\n",
    "    \n",
    "    for i, dilation in enumerate(reversed(dilations)):\n",
    "        x = layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            dilation_rate=dilation,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "    \n",
    "    decoded = layers.Conv2D(\n",
    "        1,\n",
    "        kernel_size=3,\n",
    "        padding='same',\n",
    "        activation='linear',\n",
    "        name='output'\n",
    "    )(x)\n",
    "    \n",
    "    decoder = keras.Model(decoder_input, decoded, name='decoder')\n",
    "    \n",
    "    # AUTOENCODER COMPLETO\n",
    "    autoencoder_input = keras.Input(shape=input_shape)\n",
    "    encoded = encoder(autoencoder_input)\n",
    "    decoded = decoder(encoded)\n",
    "    autoencoder = keras.Model(autoencoder_input, decoded, name='autoencoder')\n",
    "    \n",
    "    return encoder, decoder, autoencoder\n",
    "\n",
    "print(\"‚úÖ Funci√≥n build_autoencoder definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195869cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(y_true, y_pred):\n",
    "    \"\"\"Loss MSE ponderada por pesos espaciales.\"\"\"\n",
    "    squared_diff = tf.square(y_true - y_pred)\n",
    "    weighted_squared_diff = squared_diff * weights_tensor\n",
    "    return tf.reduce_mean(weighted_squared_diff)\n",
    "\n",
    "print(\"‚úÖ Loss function definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config, experiment_id):\n",
    "    \"\"\"Ejecutar un experimento completo con configuraci√≥n dada.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üß™ EXPERIMENTO {experiment_id}: {config['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Build model\n",
    "    encoder, decoder, autoencoder = build_autoencoder(\n",
    "        latent_dim=config['latent_dim'],\n",
    "        dilations=config['dilations'],\n",
    "        l2_reg=config.get('l2_reg', 0.0001)\n",
    "    )\n",
    "    \n",
    "    autoencoder.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=weighted_mse_loss\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Modelo construido: latent_dim={config['latent_dim']}, dilations={config['dilations']}\")\n",
    "    \n",
    "    # 2. Train autoencoder\n",
    "    X_train_single = X_train[:, -1, :, :, :]  # √öltimo frame\n",
    "    X_val_single = X_val[:, -1, :, :, :]\n",
    "    \n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = autoencoder.fit(\n",
    "        X_train_single, X_train_single,\n",
    "        validation_data=(X_val_single, X_val_single),\n",
    "        epochs=config.get('epochs', 100),\n",
    "        batch_size=config.get('batch_size', 32),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Entrenamiento completado en {train_time:.1f}s\")\n",
    "    print(f\"   Final train loss: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"   Final val loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "    print(f\"   √âpocas: {len(history.history['loss'])}\")\n",
    "    \n",
    "    # 3. Encode to latent space\n",
    "    latent_train = encoder.predict(X_train[:, -1], verbose=0)\n",
    "    latent_test = encoder.predict(X_test[:, -1], verbose=0)\n",
    "    \n",
    "    # 4. Train DMD\n",
    "    X_snapshots = latent_train[:-1].T\n",
    "    Y_snapshots = latent_train[1:].T\n",
    "    \n",
    "    svd_rank = config.get('svd_rank', 0.99)\n",
    "    dmd = DMD(svd_rank=svd_rank)\n",
    "    dmd.fit(X_snapshots)\n",
    "    \n",
    "    n_modes = dmd.modes.shape[1]\n",
    "    eigs_magnitude = np.abs(dmd.eigs)\n",
    "    n_stable = np.sum(eigs_magnitude < 1.0)\n",
    "    \n",
    "    print(f\"‚úÖ DMD entrenado: {n_modes} modos, {n_stable} estables ({100*n_stable/n_modes:.1f}%)\")\n",
    "    \n",
    "    # 5. Forecasting 1-step\n",
    "    z0 = latent_test[0]\n",
    "    Lambda = np.diag(dmd.eigs)\n",
    "    Phi = dmd.modes\n",
    "    Phi_inv = np.linalg.pinv(Phi)\n",
    "    A_dmd = Phi @ Lambda @ Phi_inv\n",
    "    \n",
    "    latent_forecasts = []\n",
    "    z_current = z0\n",
    "    for _ in range(len(latent_test)):\n",
    "        z_next = A_dmd @ z_current\n",
    "        latent_forecasts.append(z_next.real)\n",
    "        z_current = z_next\n",
    "    \n",
    "    latent_forecasts = np.array(latent_forecasts)\n",
    "    \n",
    "    # 6. Decode to spatial\n",
    "    spatial_forecasts = decoder.predict(latent_forecasts, verbose=0)\n",
    "    \n",
    "    # 7. Desnormalizar\n",
    "    spatial_forecasts_flat = spatial_forecasts.reshape(-1, n_lat * n_lon)\n",
    "    spatial_forecasts_real = scaler.inverse_transform(spatial_forecasts_flat)\n",
    "    spatial_forecasts_real = spatial_forecasts_real.reshape(-1, n_lat, n_lon, 1)\n",
    "    \n",
    "    y_test_flat = y_test.reshape(-1, n_lat * n_lon)\n",
    "    y_test_real = scaler.inverse_transform(y_test_flat)\n",
    "    y_test_real = y_test_real.reshape(-1, n_lat, n_lon, 1)\n",
    "    \n",
    "    # 8. M√©tricas\n",
    "    mae = np.mean(np.abs(spatial_forecasts_real - y_test_real))\n",
    "    rmse = np.sqrt(np.mean((spatial_forecasts_real - y_test_real) ** 2))\n",
    "    \n",
    "    print(f\"‚úÖ Forecasting completado:\")\n",
    "    print(f\"   MAE:  {mae:.3f} mm/d√≠a\")\n",
    "    print(f\"   RMSE: {rmse:.3f} mm/d√≠a\")\n",
    "    \n",
    "    # 9. Guardar resultados\n",
    "    results = {\n",
    "        'experiment_id': experiment_id,\n",
    "        'config': config,\n",
    "        'train_time': train_time,\n",
    "        'train_loss': history.history['loss'][-1],\n",
    "        'val_loss': history.history['val_loss'][-1],\n",
    "        'epochs': len(history.history['loss']),\n",
    "        'n_modes': n_modes,\n",
    "        'n_stable_modes': n_stable,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Experimento completado en {time.time() - start_time:.1f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Funci√≥n run_experiment definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4d588",
   "metadata": {},
   "source": [
    "## **3. Definir Grid de Experimentos**\n",
    "\n",
    "Combinaciones de hiperpar√°metros a explorar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid de experimentos\n",
    "experiments = []\n",
    "\n",
    "# Experimento 1: Baseline (configuraci√≥n actual)\n",
    "experiments.append({\n",
    "    'name': 'Baseline',\n",
    "    'latent_dim': 64,\n",
    "    'dilations': [1, 2, 4, 8],\n",
    "    'svd_rank': 0.99,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32\n",
    "})\n",
    "\n",
    "# Experimentos 2-5: Variar latent_dim\n",
    "for latent_dim in [32, 128, 256]:\n",
    "    experiments.append({\n",
    "        'name': f'LatentDim_{latent_dim}',\n",
    "        'latent_dim': latent_dim,\n",
    "        'dilations': [1, 2, 4, 8],\n",
    "        'svd_rank': 0.99,\n",
    "        'epochs': 100,\n",
    "        'batch_size': 32\n",
    "    })\n",
    "\n",
    "# Experimentos 6-9: Variar SVD rank\n",
    "for svd_rank in [0.90, 0.95, 1.0]:\n",
    "    experiments.append({\n",
    "        'name': f'SVDRank_{svd_rank:.2f}',\n",
    "        'latent_dim': 64,\n",
    "        'dilations': [1, 2, 4, 8],\n",
    "        'svd_rank': svd_rank,\n",
    "        'epochs': 100,\n",
    "        'batch_size': 32\n",
    "    })\n",
    "\n",
    "# Experimentos 10-11: Variar dilations\n",
    "experiments.append({\n",
    "    'name': 'Dilations_1_3_9_27',\n",
    "    'latent_dim': 64,\n",
    "    'dilations': [1, 3, 9, 27],\n",
    "    'svd_rank': 0.99,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32\n",
    "})\n",
    "\n",
    "experiments.append({\n",
    "    'name': 'Dilations_1_2_4',\n",
    "    'latent_dim': 64,\n",
    "    'dilations': [1, 2, 4],\n",
    "    'svd_rank': 0.99,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32\n",
    "})\n",
    "\n",
    "# Experimentos 12-14: Variar epochs\n",
    "for epochs in [50, 150]:\n",
    "    experiments.append({\n",
    "        'name': f'Epochs_{epochs}',\n",
    "        'latent_dim': 64,\n",
    "        'dilations': [1, 2, 4, 8],\n",
    "        'svd_rank': 0.99,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': 32\n",
    "    })\n",
    "\n",
    "# Experimentos combinados (mejores de cada categor√≠a)\n",
    "experiments.append({\n",
    "    'name': 'Combined_LargeDim_HighRank',\n",
    "    'latent_dim': 128,\n",
    "    'dilations': [1, 2, 4, 8],\n",
    "    'svd_rank': 1.0,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32\n",
    "})\n",
    "\n",
    "experiments.append({\n",
    "    'name': 'Combined_SmallDim_LowRank',\n",
    "    'latent_dim': 32,\n",
    "    'dilations': [1, 2, 4, 8],\n",
    "    'svd_rank': 0.90,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Grid de experimentos definido: {len(experiments)} configuraciones\")\n",
    "print(f\"\\nüìã Resumen:\")\n",
    "for i, exp in enumerate(experiments, 1):\n",
    "    print(f\"   {i:2d}. {exp['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f502f30",
   "metadata": {},
   "source": [
    "## **4. Ejecutar Experimentos**\n",
    "\n",
    "‚ö†Ô∏è **NOTA**: Esto tomar√° varias horas. Se recomienda ejecutar en sesiones separadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf218e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar todos los experimentos\n",
    "all_results = []\n",
    "\n",
    "for i, config in enumerate(experiments, 1):\n",
    "    try:\n",
    "        results = run_experiment(config, experiment_id=i)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Guardar resultados incrementalmente\n",
    "        with open(RESULTS_DIR / 'experiments_results.pkl', 'wb') as f:\n",
    "            pickle.dump(all_results, f)\n",
    "        \n",
    "        print(f\"üíæ Progreso guardado: {i}/{len(experiments)} experimentos\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en experimento {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüéâ TODOS LOS EXPERIMENTOS COMPLETADOS: {len(all_results)}/{len(experiments)} exitosos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8cb3ba",
   "metadata": {},
   "source": [
    "## **5. An√°lisis de Resultados**\n",
    "\n",
    "Visualizar y comparar todos los experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e1de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar resultados (si se ejecut√≥ en sesi√≥n anterior)\n",
    "# with open(RESULTS_DIR / 'experiments_results.pkl', 'rb') as f:\n",
    "#     all_results = pickle.load(f)\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df_results = pd.DataFrame([\n",
    "    {\n",
    "        'experiment_id': r['experiment_id'],\n",
    "        'name': r['config']['name'],\n",
    "        'latent_dim': r['config']['latent_dim'],\n",
    "        'svd_rank': r['config']['svd_rank'],\n",
    "        'dilations': str(r['config']['dilations']),\n",
    "        'epochs': r['config']['epochs'],\n",
    "        'train_time': r['train_time'],\n",
    "        'train_loss': r['train_loss'],\n",
    "        'val_loss': r['val_loss'],\n",
    "        'n_modes': r['n_modes'],\n",
    "        'mae': r['mae'],\n",
    "        'rmse': r['rmse']\n",
    "    }\n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "# Ordenar por MAE\n",
    "df_results = df_results.sort_values('mae')\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"üìä RESULTADOS DE TODOS LOS EXPERIMENTOS\")\n",
    "print(\"=\"*100)\n",
    "print(df_results[['experiment_id', 'name', 'latent_dim', 'svd_rank', 'mae', 'rmse', 'train_time']].to_string())\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Guardar CSV\n",
    "df_results.to_csv(RESULTS_DIR / 'experiments_summary.csv', index=False)\n",
    "print(f\"\\nüíæ Resultados guardados: {RESULTS_DIR / 'experiments_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bcc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 mejores configuraciones\n",
    "print(\"\\nüèÜ TOP 5 MEJORES CONFIGURACIONES (por MAE):\")\n",
    "print(\"=\"*80)\n",
    "top5 = df_results.head(5)\n",
    "for i, row in enumerate(top5.itertuples(), 1):\n",
    "    emoji = 'ü•á' if i == 1 else 'ü•à' if i == 2 else 'ü•â' if i == 3 else ''\n",
    "    print(f\"{emoji} #{row.experiment_id}: {row.name}\")\n",
    "    print(f\"   MAE: {row.mae:.3f} mm/d√≠a, RMSE: {row.rmse:.3f} mm/d√≠a\")\n",
    "    print(f\"   Latent: {row.latent_dim}, SVD rank: {row.svd_rank}, Modos: {row.n_modes}\")\n",
    "    print(f\"   Train time: {row.train_time:.1f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521efa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 1. MAE vs Latent Dim\n",
    "latent_dims = df_results['latent_dim'].unique()\n",
    "mae_by_latent = df_results.groupby('latent_dim')['mae'].mean()\n",
    "axes[0].bar(latent_dims, mae_by_latent, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Latent Dimension', fontsize=11)\n",
    "axes[0].set_ylabel('MAE (mm/d√≠a)', fontsize=11)\n",
    "axes[0].set_title('MAE vs Latent Dimension', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MAE vs SVD Rank\n",
    "svd_ranks = sorted(df_results['svd_rank'].unique())\n",
    "mae_by_svd = df_results.groupby('svd_rank')['mae'].mean()\n",
    "axes[1].plot(svd_ranks, mae_by_svd, marker='o', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('SVD Rank', fontsize=11)\n",
    "axes[1].set_ylabel('MAE (mm/d√≠a)', fontsize=11)\n",
    "axes[1].set_title('MAE vs SVD Rank', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. MAE vs RMSE scatter\n",
    "axes[2].scatter(df_results['mae'], df_results['rmse'], alpha=0.6, s=100)\n",
    "axes[2].set_xlabel('MAE (mm/d√≠a)', fontsize=11)\n",
    "axes[2].set_ylabel('RMSE (mm/d√≠a)', fontsize=11)\n",
    "axes[2].set_title('MAE vs RMSE', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Train time vs MAE\n",
    "axes[3].scatter(df_results['train_time'], df_results['mae'], alpha=0.6, s=100)\n",
    "axes[3].set_xlabel('Train Time (s)', fontsize=11)\n",
    "axes[3].set_ylabel('MAE (mm/d√≠a)', fontsize=11)\n",
    "axes[3].set_title('Efficiency: Train Time vs MAE', fontsize=12, fontweight='bold')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. N Modes vs MAE\n",
    "axes[4].scatter(df_results['n_modes'], df_results['mae'], alpha=0.6, s=100)\n",
    "axes[4].set_xlabel('Number of DMD Modes', fontsize=11)\n",
    "axes[4].set_ylabel('MAE (mm/d√≠a)', fontsize=11)\n",
    "axes[4].set_title('DMD Modes vs MAE', fontsize=12, fontweight='bold')\n",
    "axes[4].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Top 10 experiments bar chart\n",
    "top10 = df_results.head(10)\n",
    "axes[5].barh(range(len(top10)), top10['mae'], alpha=0.7, edgecolor='black')\n",
    "axes[5].set_yticks(range(len(top10)))\n",
    "axes[5].set_yticklabels([f\"{row.experiment_id}: {row.name}\" for row in top10.itertuples()], fontsize=9)\n",
    "axes[5].set_xlabel('MAE (mm/d√≠a)', fontsize=11)\n",
    "axes[5].set_title('Top 10 Experiments', fontsize=12, fontweight='bold')\n",
    "axes[5].invert_yaxis()\n",
    "axes[5].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'hyperparameter_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"üíæ Guardado: {FIG_DIR / 'hyperparameter_analysis.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b2245",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Conclusiones**\n",
    "\n",
    "Este notebook permite:\n",
    "\n",
    "1. **Exploraci√≥n sistem√°tica** de hiperpar√°metros\n",
    "2. **Comparaci√≥n objetiva** de configuraciones\n",
    "3. **Identificaci√≥n de trade-offs** (performance vs tiempo)\n",
    "4. **Selecci√≥n de configuraci√≥n √≥ptima** para producci√≥n\n",
    "\n",
    "**Pr√≥ximos pasos:**\n",
    "- Re-entrenar modelo √≥ptimo con m√°s √©pocas\n",
    "- Validar configuraci√≥n √≥ptima en a√±os 2019-2021\n",
    "- Registrar mejor modelo en MLflow\n",
    "\n",
    "**Nota**: Este notebook puede tardar 2-4 horas en ejecutarse completamente."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
